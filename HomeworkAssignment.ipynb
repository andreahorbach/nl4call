{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Assignment\n",
    "\n",
    "In this homework assignment you will try out some small examples for language learning exercises yourself using NLP. Even if you have little or no programming experience so far, you can run the examples and inspect the output. Todo so, click \"Run\" in the panel above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we have to import some libraries we want to use.\n",
    "import nltk\n",
    "nltk.download('punkt')\n",	
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Create a Preposition Exercise.\n",
    "\n",
    "In this first example, we want to create a gap-filling exercise that transform a sentence into a gap-filling exercise, where each preposition is replaced by a gap. To achieve this, we have to split the text into individual tokens, assign a part-of-speech tag for every token and then replace those tokens whose POS is 'article' by a gap.\n",
    "Run the cell to see how it is done. \n",
    "\n",
    "You can experiment with the exercise in two ways: \n",
    "- By changing the text.\n",
    "- By searching or a different part-of-speech. The tagset that is used by the POS tagger can be found under https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],  
   "source": [
    "# input text\n",
    "text = \"She is waiting for her friends at the station.\"\n",
    "\n",
    "# First, the text is split into individual words\n",
    "tokens = word_tokenize(text)\n",
    "# Let's print them to see what is happening\n",
    "print(\"Tokens: \",tokens,\"\\n\")\n",
    "\n",
    "# Next, each word is assigned its part-of-speech\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "# Print also the POS tags\n",
    "print(\"POS: \",pos_tags, \"\\n\")\n",
    "\n",
    "# Now, we construct the exercise sentence. We iterate through all POS tags. \n",
    "#If thePOS tag is 'IN' for preposition, we replace add a gap, otherwise, we add the word itself.\n",
    "output = ''\n",
    "for (pos) in (pos_tags):\n",
    "    if (pos[1] == \"IN\"):\n",
    "        output += \"___ \"\n",
    "    else:\n",
    "        output += pos[0]+\" \"\n",
    "\n",
    "print(\"Exercise: \",output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Create an Irregular Verb Exercise.\n",
    "For the second exercise, let's assume we want practice irregular verbs in English. We will create a gap-fill exercise again, butstudents should additionally get a hint which verb they should fill in. As before, we need to tokenize the text into words and assign each word a POS tag to filter for verbs. (In the Penn treebank tagset e use here, verbs in past tense are marked as \"VBD\".)\n",
    "Additionally, to create the hint for the student, we want to extract the base form for every word using a lemmatizer. Run the cell to try the exercise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [], 
    "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text = \"Mary woke up at 7am, drank a coffee, bought a croissant and then wrote an e-mail to her aunt.\"\n",
    "\n",
    "# First, the text is split into individual words, you know this step already.\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n",
    "\n",
    "# Next, each word is assigned its part-of-speech, you know that too.\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print(\"POS: \",pos_tags,\"\\n\")\n",
    "\n",
    "\n",
    "# Now, we iterate through all words, searching for past tense verbs\n",
    "output = ''\n",
    "for (pos) in (pos_tags):\n",
    "    if (pos[1] == \"VBD\"):\n",
    "        # look up the lemma of each past tense verb\n",
    "        lemma = lemmatizer.lemmatize(pos[0], \"v\")\n",
    "        output += \"___(\"+lemma+\") \"\n",
    "    else:\n",
    "        output += pos[0]+\" \"\n",
    "\n",
    "print(\"Exercise: \",output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
